---
layout: post
title: "ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately"
author: taekyoon.choi
categories: [paper]
tags: [electra]

---

---
## ELECTRA

ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) 모델은 기존 BERT보다 빠른 속도록 학습을 하면서 더 좋은 성능을 내는 모델로 소개 되었습니다.
기존의 BERT의 Masked Token Prediction 방식을 활용하지 않고 Replaced Token Detection 방식으로 downstream task를 더 잘하는 Pretrained Model에 대해서 소개하는데요.
Electra-base 기준 GLUE 점수가 BERT-base에 비해 약 3점정도가 높은 것을 확인할 수 있습니다. 
하지만, 이 논문에서는 BERT보다 성능이 좋다는 점 보다는 학습 속도측면에서 강점이 있는 점을 더 어필하였습니다.
NVIDIA V100 GPU 1대만으로 4일간 학습하면 Bert-small 모델보다 더 나은 성능의 퍼포먼스를 보여 빠른 학습과 성능 괜찮은 Language PLM을 소개하였습니다.

### 소개

이 논문에서는 BERT의 MLM(Masked Language Model)의 학습 비용이 많이 드는 점을 들어 자신들이 제안할 Replaced Token Detection을 이야기 하고자 하고 있습니다.
Replaced Token Detection은 말 그대로 텍스트에 있는 토큰들 중 바뀐 토큰을 탐지하는 테스크인데요. 
여기서 Replace 되는 입력 텍스트는 MLM 모델을 활용하여 생성된 토큰들을 활용하였습니다.
이와 같은 방식으로 ELECTRA 모델은 기존의 BERT, RoBERTa 보다 빠른 시간에 더 나은 fine-tune 성능을 보였고 이를 NLP downstream 테스크를 대표하는 GLUE score로 표현했습니다.

<img width="854" alt="" src="https://user-images.githubusercontent.com/16576921/127728040-8963e70c-0c6b-4c6d-9f2e-321418bdc04c.png">


### 방법

ELECTRA 모델에서 핵심은 Replaced Token Detection (RTD)에 있습니다. 
RTD방식을 가지고 Pretrained Model 모델을 학습 하기 위해서는 입력 텍스트 토큰에 Replaced 된 토큰을 바꿔 넣어줘야 합니다.
이 토큰을 변경하는 방식을 ELECTRA에서는 다음의 그림과 같은 방식을 활용하였습니다.

<img width="846" alt="스크린샷 2021-07-31 오후 1 02 16" src="https://user-images.githubusercontent.com/16576921/127728061-92f5c8f1-d8e9-4b35-b40c-9df898470274.png">

먼저 Generator에서 MLM 방식으로 텍스트 토큰에 mask를 씌워 corrupted 토큰을 생성합니다.
그리고 입력 토큰과 다른 토큰 위치에 대해서만 replaced 되었는지 판단하는 학습을 Discriminator에서 진행합니다.
여기서 Discriminator가 하는 역할이 Replaced Token Detection 입니다.
수식으로 표현을 하자면 이렇습니다. 

<img width="532" alt="스크린샷 2021-07-31 오후 1 03 01" src="https://user-images.githubusercontent.com/16576921/127728074-ec0d3cd5-9a6c-4d8e-8ba0-b17d0a3c1517.png">

<img width="300" alt="스크린샷 2021-07-31 오후 1 03 14" src="https://user-images.githubusercontent.com/16576921/127728077-9e3d6154-c74e-459d-b66c-0c4ce5289c7a.png">

여기서 Generator에는 softmax로 토큰을 생성하는 수식으로 나와있지만 실제 구현에서는 Gumbel softmax를 하여 토큰 생성 시 노이즈를 줍니다.
위 학습하는 모델에 대한 학습 Loss를 구하자면 다음과 같습니다.

<img width="655" alt="스크린샷 2021-07-31 오후 1 03 50" src="https://user-images.githubusercontent.com/16576921/127728085-46b87cd1-498f-4c28-ae69-a116fb17c524.png">

<img width="842" alt="스크린샷 2021-07-31 오후 1 04 10" src="https://user-images.githubusercontent.com/16576921/127728091-df01da79-12f6-4702-b336-951e50dacd3d.png">

모델 학습 구조를 보면 Generator와 Discriminator 형태로 되어있어 GAN과 같아 보일 수 있지만 GAN의 방식을 따를뿐 GAN은 아닙니다.
왜냐하면 학습 시 Discriminator에서 Generator로 직접 backpropagate이 어렵기 때문입니다. 
논문에서는 이런 이슈가 있기에 강화학습 방식을 활용하여 Generator에 backpropagate가 될 수 있도록 시도하였지만 성능은 좋지 않았다 합니다.
그래서 GAN과 같은 방식을 사용하지 않고 Maximum Likelihood 방식으로 학습을 합니다. 
최종 Loss 수식은 아래와 같습니다.

<img width="365" alt="스크린샷 2021-07-31 오후 1 04 49" src="https://user-images.githubusercontent.com/16576921/127728105-283f2fb6-e211-4c95-9e10-8fbb6c9d8a18.png">

### 실험 구성

ELECTRA 모델은 GLUE와 SQuAD 데이터를 벤치마크로 평가했습니다. (구체적인 평가 항목은 생략하겠습니다.)
평가를 위해 동일한 데이터셋으로 학습한 BERT을 활용했다합니다. (Wikipedia와 BookCorpus등 총 3.3B 토큰을 활용했습니다.)
Large 모델에서는 XLNet에서 추가로 활용한 데이터를 추가하여 학습했다 합니다. (ClueWeb, CommonCrawl, Gigaword등 총 33B이 추가되었습니다.)
모델은 기존 BERT와 동일한 하이퍼파라메터를 활용하고 Fine-tuning을 위해 마지막 레이어에 Linear Classifier를 추가하였습니다. (SQuAD의 경우 시작과 끝 지점을 예측하기 위해 좀 더 복잡한 방법을 활용했습니다.)

### 확장 실험들

ELECTRA 모델을 만들면서 여러가지로 성능 개선을 하고자 한 실험들이 있었습니다. 
이 방식들도 BERT-base와 동일한 모델과 데이터셋으로 진행했습니다.

#### Weight Sharing

이 실험에서는 Generator와 Discriminator의 파라메터를 공유하는 방식입니다. 
그렇기 때문에 Generator와 Discriminator의 모든 파라메터 크기는 동일해야 합니다. (Generator와 Discriminator의 마지막 layer는 각각 서로 다르게 연결 합니다.)
학습 500k 에서 파라메터를 공유한 방식과 아닌 방식의 GLUE 점수를 비교했는데 각각 83.6, 84.3 으로 나와 공유한 방식이 더 나은 점수로 나오는 것을 확인할 수 있었다 합니다.
논문에서는 MLM 학습과 RTD 학습이 하나의 Encoder 모델에 영향을 받기 때문에 성능이 오른다고 추측하고 있습니다.
하지만 Generator와 Discriminator가 서로 같은 크기의 파라메터로 있어야 한다는 점이 학습 속도의 한계라 이야기 하고 있습니다.

#### Smaller Generators

위에 소개한 방식은 Generator와 Discriminator의 크기가 같아 학습이 2배로 걸린다는 한계가 있었습니다. 
여기 실험에서는 Generator의 크기를 줄여 학습하는 한 스텝의 연산 수를 적게하고자 합니다.
구체적으로 여기서는 Generator 각 Layer의 파라메터 사이즈를 줄입니다. (대신 다른 하이퍼파라메터는 유지합니다.) 
Generator의 사이즈에 따른 GLUE 스코어를 탐색을 하였는데 discriminator 크기의 1/4에서 1/2가 가장 좋은 성능을 보입니다.
아래 그림에서 Generator size에 따라 GLUE 점수의 성능 변화가 어떤지 보여줍니다.
여기서 모든 모델은 500k 스텝까지만 학습했습니다.

<img width="480" alt="스크린샷 2021-07-31 오후 1 07 49" src="https://user-images.githubusercontent.com/16576921/127728164-7074da45-436d-461b-8273-018900bc54b8.png">

논문에서는 Generator의 생성 성능이 좋으면 discriminator이 학습하는데 어려움을 겪는데 그럼에 따라 학습을 효율적으로 하는데 방해합니다. 
그럼에도 여기서 강조하고 싶은 것은 Discriminator는 데이터만을 가지고 학습하는 것이 아닌 Generator를 통해 학습하는 것이 나을 것이라 합니다.

#### Training Alogorithms

논문에서는 여러 학습 알고리즘을 활용해서 학습을 했었지만 좋은 결과를 보지는 못했다 하면서 다음과 2-Stage 방식과 Joint Training 학습 방식을 실험했습니다.
위에 소개한 두 방식과는 별개로 2 단계를 거쳐 학습을 할 수 있도록 시도했습니다.

1. Generator를 MLM 방식으로 학습한다
2. Discriminator의 weight 초기화를 Generator의 weight로 설정하고 RTD 학습을 한다. 이 때 Generator의 weight는 freeze를 한다. (Discriminaotr의 weight 초기화 방식은 Generator와 파라메터 사이즈가 동일한 경우에 한해서 적용)

위 에서 2 단계의 방식은 Generator와 Discriminator의 파라메터 사이즈가 동일해야 가능합니다. 
Discriminator 사이즈가 달라진다면 Genrator의 weight를 가지고 weight 초기화를 하지 못하는데 이런 상황에서 학습하는 것은 Generator의 학습 차이가 있기 때문에 Discriminator의 학습이 실패하는 경우도 있다 합니다.
결국 Joint Training을 가지고 학습을 하게 되는데요. 이 방식은 Generator와 Discriminator를 둘다 weight 초기화가 된 상태에서 같이 MLM과 RTD 학습을 합니다.
이 경우에는 초반에 Generator가 성능이 좋지 않지만 가면 갈 수록 나아지는 것을 확인할 수 있었다 합니다.
(추가로 논문에서는 reinforced learning을 통해 GAN 학습 방식을 시도했다 합니다. 이 부분은 따로 소개하지 않겠습니다.)

<img width="368" alt="스크린샷 2021-07-31 오후 1 07 56" src="https://user-images.githubusercontent.com/16576921/127728166-3874b1d0-2c75-4eab-9a4a-c2cb79e0e513.png">

각 실험에 대한 결과는 위 그래프와 같이 보이고 있습니다. BERT에 비해서 더 나은 성능들을 보이고 있지만 여기서 가장 나아보이는 실험방법은 Joint Training 방식 입니다. (ELECTRA라 표기 된 것이 Joint Training 방식입니다.)
GAN을 활용한 Adversarial training의 경우 Joint Training 만큼 나오지 못한 부분에 대해서 다음과 같이 설명하고 있습니다.

1. Adversarial Generator는 MLM을 하는데 더 안좋은 성능을 보인다. MLE의 경우 65% accuracy를 보이지만 58% 정도의 성능을 보인다. 이 점은 reinforced learning 방식으로 인해 안좋은 샘플링이 되기 때문이라 본다.
2. 학습 시 Generator에서 single token에 probabilty가 몰리기 때문에 low-entropy가 전달된다.


#### Small Models

ELECTRA 논문은 앞서 말씀드렸다시피 학습 속도 측면에서 효율적이다는 것을 강조하고 있습니다. Small Model은 작은 모델이면서 빠른 속도로 학습이 가능하다는 점을 가장 잘 보여주는 부분이라 생각하는데요.
여기서는 BERT-small을 벤치마크로 비교하여 ELECTRA 성능이 더 효율적이다는 것을 표현하고 있습니다. ELECTRA 모델도 역시 BERT-small과 동일한 하이퍼파라메터로 셋팅을 하였고 batch size와 sequence length를 1/4로 줄였습니다.
여기서는 ELMo와 GPT등의 모델들을 같이 두고 GLUE 성능 점수를 비교하였습니다.

<img width="863" alt="스크린샷 2021-07-31 오후 1 08 44" src="https://user-images.githubusercontent.com/16576921/127728171-26eba494-1c64-4406-9c43-f48ee951fc1f.png">

위 성능 테이블에서 확인했다시피 여느 모델들과 비교했을 떄 ELECTRA-small 모델이 파라메터가 BERT-small과 동일하면서 빠른시간 안에 성능이 더 좋아지는 것을 확인할 수 있습니다.
테이블에 학습시간이 나와 있는데 V100 GPU 한 장으로 반나절이면 이미 BERT-small의 GLUE 성능보다 좋아질 수 있다는 점을 보여주고 있습니다.

#### Large Model

이 실험에서는 ELECTRA-large 모델을 가지고 RTD방식으로 학습을 했을 때 Large 모델을 실험했을 때에도 효율적인지를 살펴보는 실험입니다.
여기서는 Small Model 실험과 다르게 학습 데이터 셋을 XLNet 모델 학습에 활용한 데이터를 가지고 학습했습니다.
그리고 ELECTRA 모델 학습을 400K 스텝과 1.75M 스텝 시점에 대한 모델을 두고 성능 비교를 하는데요. 주로 BERT와 RoBerta와 비교를 하기위한 목적입니다.

<img width="863" alt="스크린샷 2021-07-31 오후 1 09 01" src="https://user-images.githubusercontent.com/16576921/127728175-54788546-170c-446c-a2ec-4febadf13bf4.png">

보시다시피 ELECTRA-400k 모델이 RoBerta와 XLNet 모델보다 더 낫거나 비슷한 점수를 보이는 점에서 학습 속도가 빠르다는 점을 확인할 수 있습니다.
실제로는 BERT-large 모델과 비교를 해야하는데 RoBerta-100K 모델에 비해 성능이 떨어져 있어 비교가 어려운것 같아보입니다. 
(여기서는 Roberta학습 데이터를 가지고 BERT를 학습하다보니 RoBerta에 이점이 생긴게 아닌가라는 이야기를 합니다. 아무튼 BERT-large에 비해 성능이 좋다고 이야기 하기에는 검증이 더 필요해 보입니다.)
아래 테이블에서도 역시 ELECTRA가 GLUE-test 데이터에 대해서도 좋은 성능을 낸다는 점을 보이고 있습니다.

<img width="865" alt="스크린샷 2021-07-31 오후 1 09 49" src="https://user-images.githubusercontent.com/16576921/127728190-4e31e1fd-f6e1-4819-ae22-7f5583fcfe24.png">

(논문에서는 SQuAD에 대한 내용에 대해서도 설명을 하고 있지만 이 내용은 생략하도록 하겠습니다.)

#### Efficiency Analysis

논문에서는 token masking이 효율적인 학습 방법이 아니다 라는 점을 주장 하면서 ELECTRA에 대한 모델 설명을 하고 있는데요.
3 가지 실험을 통해 ELECTRA가 어떻게 좋은 성능을 낼 수 있었는지 이야기 하고 있습니다.

- ELECTRA 15%: Discriminator 학습 시 loss를 masking을 한 15%의 토큰에 한해서만 학습
- Replace MLM: Discriminator 학습 시 Mask로 학습하지 않고 Generator에서 생성된 토큰으로 학습. 이 점은 Mask를 한 것과 하지 않을 것의 차이를 통해 ELECTRA의 이점을 찾기 위한 것임
- All-Tokens MLM: Replace MLM과 동일한 학습인데 여기서는 모든 토큰을 Generator에서 생성된 토큰으로 학습

위 실험에 대한 성능은 다음과 같습니다.

<img width="869" alt="스크린샷 2021-07-31 오후 1 10 16" src="https://user-images.githubusercontent.com/16576921/127728197-e8d1fe32-bfa4-4919-b66e-69169f644e9c.png">

ELECTRA 15%가 BERT에 비해 조금 더 나은 성능을 보이고 있고 All-Tokens MLM의 경우 BERT에 비해 좋은 성능 차를 보이고 있고 ELECTRA의 성능과 거의 가까운 것을 확인할 수 있습니다.
여기서 All-Tokens MLM에서 가장 이야기 하고 싶은 부분은 BERT에서 활용한 mask 토큰으로 인해 발생한 pre-training과 fine-tuning의 학습 불일치로 성능 차이가 난다는 점이지 않을까 싶습니다.
그리고 mask가 없는 것이 pre-training과 fine-tuning의 학습 불일치를 줄여 훨씬 더 좋은 성능을 냈다는 점을 보이고 있습니다.

다른 실험으로는 BERT에 비해 ELECTRA가 효과를 낼 수 있는 지점을 이야기 하는데요.
여러가지로 파라메터 실험을 통해 보인 결과는 아래 그래프와 같습니다.

<img width="851" alt="스크린샷 2021-07-31 오후 1 11 14" src="https://user-images.githubusercontent.com/16576921/127728221-8910b461-2efe-4b53-a1d4-6d561fa38d45.png">

ELECTRA는 모델이 작을 수록 BERT에 비해 빠른 학습에 좋은 성능을 보이고 있는 점을 보이고 있습니다. 
이 점을 두고 봤을 때 ELECTRA가 parameter에 대한 효율성이 더 있다고 이야기 할 수 있다고 하지만 보다 분석이 필요하다는 이야기도 하고 있습니다.


### 정리

ELECTRA 모델은 Replaced Token Detection이라는 방식으로 Pre-trained Language Model(PLM)을 학습합니다. 이 학습 방식으로 다른 PLM보다 훨씬 더 빠른 속도에 좋은 성능을 낼 수 있음을 보이고 있는데요.
이렇게 성능이 좋을 수 있는 점은 기존 BERT에서 학습했을 때 활용한 mask 토큰으로 인한 pretrain과 fine-tuning에 대한 학습 불일치라 볼 수 있습니다.

이를 통해 기존 Bert 뿐만 아니라 RoBERTa XLNet 등과 비교 했을 때도 빠른 시간안에 괜찮거나 좋은 성능을 낸다는 점을 GLUE를 통해 보여주고 있습니다.
실험 결과들을 보면서 굉장히 실용적인 부분은 Small Model에서 하루 만에 PLM을 만들 수 있다는 점을 들기도 합니다. 분명히 Small Model을 만들어 모델 생산을 하는데는 분명히 이점이 있어보입니다.

다만 논문에 이러한 모델을 찾기 까지 많은 과정들이 있었다는 점들이 있었다는 것을 여러 실험 케이스들을 소개하면서 보여주고 있는데요. 그만큼 이 모델을 좋은 성능으로 만들기 까지 까다로운 과정들이 있었지 않았나라는 생각도 한 편으로 듭니다.
또한 Generator의 생성 퀄리티에 따라 Discriminator의 성능도 달라지는 점도 고려해야할 부분이라 생각이 드는데요. 기존 MLM 방식의 Transformer 모델에 비해서는 조금 더 신경써야 하는 하이퍼파라메터가 생긴 것 같습니다. 이 점은 모델을 만들 시에 비용이 조금 더 드는 부분이 아닐까 생각이 듭니다.
