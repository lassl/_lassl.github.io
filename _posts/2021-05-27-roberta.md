---
layout: post
title: RoBERTa(A Robustly Optimized BERT Pretraining Approach) 논문리뷰
author: inje.ryu
categories: [paper]
tags: [roberta]

---

# RoBERTa 논문리뷰


# 1. Introduction

- 기존에 제안된 ELMo / GPT / BERT / XLM / XLNET 등이 놀라운 성과를 보였지만, 모델의 어떤 측면이 가장 기여했는가에 대해서는 Challenging 하였음
- BERT를 통해 하이퍼파라미터 튜닝 및 Training data size 를 변경하며
여러 시도를 해본 결과, BERT가 덜 학습(Significantly undertrained) 되어 있으며 성능을 개선할 수 있었음
- 이 방법론은 "RoBERTa" 로 명명하며, 기존 BERT의 방법론을 뛰어넘는 결과를 보임

## How?

- RoBERTa는 간단한(?) 다음 방법론을 이용해 모델을 개선함
    1. 모델(Pre-trained model)을 더 오래 학습, 큰 배치 사이즈, 더 많은 Data를 활용함
    2. NSP(Next Sentence Prediction) 방법론을 활용하지 않음
    3. Pre-training 단계에서 더 긴 문장을 활용
    4. Pre-training 단계에서 Dynamically changing 마스킹을 활용함
        - 여기에 training size effect 를 비교해보고자 large dataset을 새로 수집함
- 성과
    1. Pre-training 단계에서 BERT 학습 전략 수정을 통해 downstream task 성능을 높임
        - BERT design choice & 학습 전략 변경 → 새로운 전략이 성과가 좋았음
    2. Pre-training 단계에서 더 많은 데이터를 활용해서 downstream task 성능을 향상 시킴
    3. MLM(masked language model pretraining) 에서 성능 개선을 포인트를 발견했고, 최근 제안된 방법들과 비교해서도 우수한 성능을 보임

# 2. Backgroud (BERT)

- RoBERTa의 대부분 방법론은 BERT를 기반으로 하고 있어 BERT의 주요 접근법과 유사함

## 2.1 Setup

<center><img src="https://i.imgur.com/RAmRtu5.png"></center> 
<center> BERT input representation <a href="https://arxiv.org/abs/1810.04805" target="_blank">(BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding)</a></center>

- BERT에서 제안된 input Sequence
- [CLS] x,x,x,x,x,x,x, [SEP], y,y,y,y,y,y,y,y, [EOS] 의 구조임
- $x_n$ / $y_m$ 이 각 파트의 최대 길이 일때, max(n + m) < max_seq_length임 (512)

## 2.2 Architecture

- BERT의 구조와 동일함
- 12 Encoder / 12 Attention / 768 Hidden state

## 2.3 Training Objectives

- Masked Language model (MLM) : 전체 중 15%를 Masking ( 그중 80% 마스킹, 10% 유지, 10% 랜덤으로 변경)
    - 기존 실험에서는 랜덤 마스킹 & replacement가 발생하면, 학습전체 동안 saved된 후 동일했지만, 이번에는 마스킹이 동일하지 않도록 실험을 진행함
- Next Sentence Prediction (NSP) : Next Sentence Prediction은 두 문장을 주고 두 번째 문장이 코퍼스 내에서 첫 번째 문장의 바로 다음에 오는지 여부를 예측하도록 하는 방식이며, NLI(Natural Language Inference)와 같은 다운스트림 태스크의 성과를 개선하기 위해 고안되었음

## 2.4 Optimization & Data

<center><img src="https://i.imgur.com/BTPU3sh.png"></center> 
<center> BERT Hyperparameter <a href="https://arxiv.org/abs/1810.04805" target="_blank">(BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding)</a></center>


- Adam Optimizer 사용, $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 1e-6$, $L_2 \ weight \ decay = 0.01$
- lr 은 첫번째 10,000 step까지 $1e-4$  까지 warmed up 되다가 이후 리니어하게 감소
- drop-out: 0.1 (모든 layer & attention weights)
- Pre-training 단계에서 1,000,000 번의 스탭을 학습 / 256개의 미니배치 / T=512 (tokens)
- 위키피디아 등 16GB 의 데이터셋 활용

# 3. Experimental Setup

## 3.1 Setup

- Section 2에서 주어진 hyperparameters에서 peak learning rate & warm-up step 을 제외하고는 분리해서 실험을 진행
- $\beta_2$ 파라미터를 0.98로 하면 큰 배치사이즈에서 안정성이 개선되는 부분을 확인
- 토큰의 최대 길이가 512인데, 최대 길이의 문장을 pre-training 함

## 3.2 Data

- 기존 16GB의 데이터 + 144GB → 160GB 데이터를 pre-training 단계에서 사용함
    - CC-News(76GB)
    - OpenWebText(38GB)
    - Stories(31GB)

## 3.3 Evaluation

- GLUE - 종합적인 언어이해도를 측정하는 Dataset으로 9개의 subtask로 구성
- SQuAD - context & question으로 구성된 Dataset (V1.1 & V2.0 사용)
    - V1.1: BERT와 동일하게 적용
    - V2.0: binary Classification으로 대답이 가능한지 여부만 확인
- RACE - 대규모 질의응답 Dataset으로 28,000개 문장과 100,000개 질문으로 구성

# 4. Training Procedure Analysis

- 아래 실험에서 Architecture는 동일하게 구성 BERT(base) - L=12 / H=768 / A=12 / 110M params

## 4.1 Static vs Dynamic Masking

- Static : 기존 BERT모델에서는 한번 랜덤으로 마스킹된 곳을 pre-training 단계에서 계속 활용
- Dynamic Masking : 동일 문장을 복제한 후에 10가지 다른 방법으로 마스킹 한뒤 40epoch에 걸쳐 학습
(즉, 마스킹까지 완전하게 동일한 문장은 학습동안 4번 등장)

<center><img src="https://i.imgur.com/FnWctay.png"></center> 
<center> Masking strategy benchmark <a href="https://arxiv.org/abs/1907.11692" target="_blank">(RoBERTa: A Robustly Optimized BERT Pretraining Approach)</a></center>


### Metric (median)

- SQuAD - F1
- MNLI-m, SST-2 - acc
- 기존 BERT와 차이가 없거나, 몇몇 task에서 개선되는 부분을 확인 → 이후에 계속 반영

## 4.2 Model Input Format and Next Sentence Prediction

- 기존 BERT 모델에서는 $p=0.5$의 확률로 샘플링된 문장이 동일 doc-segment에서 등장했는지 확인했으며, 이를 NSP라고 함

    → 기존 BERT에서는 NSP가 없는 경우 QNLI, MNLI, SQuAD1.1 에서 성능이 저하됨

```
NSP의 필요성에 대해서 확인해보자
```

### Definition of Input SEQ

- SEGMENT-PAIR+NSP : BERT 원본 (pair of seq & 최대 512 이하)
- SENTENCE-PAIR+NSP : Document의 문장단위로 입력 (한개의 doc의 연속된 부분이거나 다른 doc)
    - 그런데 이경우에 512보다 훨씬 짧기 때문에 batchsize를 늘림
- FULL-SENTENCES : 무조건 최대길이 512에 맞춰 input을 생성 / document를 넘어가는 경우 end of document token 삽입 (NSP loss 제거)
- DOC-Sentences : 위와 유사하지만, 문서 내에서 doc-sentence 구현 → 마지막 문장 근처에서는 Document를 넘어가지 않아 512보다 짧을 수 있음 (한번에 optimize되는 토큰 수를 유지하기 위해서 dynamic하게 배치사이즈를 조절함)

<center><img src="https://i.imgur.com/O0L83wQ.png"></center> 
<center> SEQ 전략에 따른 결과 <a href="https://arxiv.org/abs/1907.11692" target="_blank">(RoBERTa: A Robustly Optimized BERT Pretraining Approach)</a></center>

- 일단 sentence-pair를 사용하는 경우는 downstream task에서 성능이 안좋음
- 기존 BERT에서는 NSP가 중요한 point였는데, NSP loss를 없애고 문장의 길이를 채워서 input을 넣어주는 것 만으로도 **NSP와 유사하거나 더 나은 performance**를 보임
- NSP를 제외한 두 컨셉 중에서는 Doc-Sentences가 좀 더 나았지만, **batch-size를 다양하게 조절해야하기 때문에 다른 실험과의 일관성을 위해 이후 실험에서는 Full-Sentence를 활용함**

## 4.3 Batch size

- 이전 실험들을 통해서 large batch-size → improve optimization speed & lr이 적절하게 증가함에 따라 성능이 개선됨
- BERT : 256 seq batch size / 1M steps
- 2K seq batch size / 125K steps → try (배치사이즈 8배, step 1/8배)
- 8K seq batch size / 31K steps → try

<center><img src="https://i.imgur.com/F6xCwwp.png"></center> 
<center> Batch size & LR 조절에 따른 결과 <a href="https://arxiv.org/abs/1907.11692" target="_blank">(RoBERTa: A Robustly Optimized BERT Pretraining Approach)</a></center>

- 2K / 125K 조합이 조금 더 좋은 결과를 보였다
    - lr은 batch-size가 증가한만큼 반영한걸로 추측(?)되지만, 정비례는 아니고 세부적인 언급은 없음

## 4.4 Text Encoding

- 30K Character level BPE (BERT) → 50K Byte level BPE (RoBERTa)
- 기존 방식과 차이가 있고, 성능이 저하되는 케이스가 있음
- 그럼에도 불구하고 unknown token을 없앨수 있다는 컨셉(universal encoding)에 대한 이점을 활용하였고, 좀더 디테일한 비교는 미래에 숙제로 남겨둠

# 5. Result

앞선 실험의 결과물들을 종합해보자

<center><img src="https://i.imgur.com/K8niqOu.png"></center>
<center> 실험 결과 요약 <a href="https://arxiv.org/abs/1907.11692" target="_blank">(RoBERTa: A Robustly Optimized BERT Pretraining Approach)</a></center>

- 요약하면, 앞선 실험들을 종합한 결과로부터 BERT 대비 성능을 향상함
- 크게 4가지 관점의 변화가 있었으며, 이 결과물을 **R**obustly **o**ptimized **BERT** **a**pproach (RoBERTa) 라고 명명함
    1. Dynamic masking(4.1)
    2. NSP를 제외하고 Full-sentense를 제안(4.2)
    3. Large mini batch(4.3)
    4. tokenizer를 BBPE(50k)로 변경 (4.4)
- XLnet은 BERT보다 10배 이상의 데이터와 4배 이상의 training으로 나온 결과물임

→ 그래서 우리도 BERT large에 추가 데이터와 더많은 batch size+step으로 training했더니 XLnet 넘어섬

→ 여기에 마지막 모델 또한 데이터에 대해 오버피팅이 발생하지 않고 있어서 추가적인 트레이닝으로 이점을 얻을수 있어 보임

→ 최종 모델(even longer)로 섹션 3에서 제시된 데이터들에 대해서 벤치마크를 진행함

<center><img src="https://i.imgur.com/BEGW1DS.png"></center> 
<center> GLUE 벤치마크 결과 <a href="https://arxiv.org/abs/1907.11692" target="_blank">(RoBERTa: A Robustly Optimized BERT Pretraining Approach)</a></center>

## 5.1 GLUE

GLUE에 대해서는 2가지 finetuning setting을 한 후 진행함

1. Single model : 각 GLUE task에 대해서 training data만 활용하고 각 train set에 대해 학습, hyperparameter를 제한해서 다른 논문들과 유사하게 적절한 범위 내에서 선택함
    - Finetuning시에 3epoch정도만 학습하는 반면에 10epoch학습 + early stopping 사용
2. Ensemble model : 다른 벤치마크들과 비교하기위해서 single-task model의 앙상블함 
(여기서 다른 벤치마크들은 성능을 높이기 위해서 multitask finetuning 이용하는데, 우리는 안씀)
    - RTE, STS, MRPC는 pretrain모델로 시작하는 것보다는 MNLI를 학습한 후 다시 finetuning 하는것이 성능이 좋음
    - 9개중에 4개가 벤치마크 대비 높았고 평균적으로 SOTA 달성


## 5.2 SQuAD & RACE

- SQuAD 2.0 training data에만 finetuning 진행 → data augumentation 없는 Single 모델중 제일 잘나옴
- 4개 중에 어떤 문장이 답변인지 확인 → SOTA 달성

# 6. Conclusion

- 이번 논문을 통해서 BERT를 pretraining 하기위한 몇가지 학습적 디자인을 발굴
    - 모델을 더 오래 학습하고
    - 더 많은 데이터에 대해 더 큰 배치사이즈로 접근
    - NSP를 제거
    - 긴 문장(full-sentence)에 대해 학습
    - dynamic change masking
- GLUE / RACE / SQuAD에서 SOTA
- BERT는 여전히 다른 모델보다 경쟁력있다