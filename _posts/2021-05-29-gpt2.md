---
layout: post
title: GPT-2
author: 
categories: [paper]
tags: [gpt]

---

---
## GPT-2

GPT-2는 2019년에 등장한 모델이고 지금의 GPT-3 모델이 나올 수 있게 될 수 있는 근간에 대한 내용이 담겨져 있습니다.
GPT-1과 다르게 GPT-2에서는 "Language Models are Unsupervised Multitask Learners"라는 제목으로 언어 모델로 여러 가지 언어 테스크를 할 수 있는 모델로 소개가 되어있는데 당시에 어떤 생각을 가지고 지금의 GPT-3가 나올 수 있었는지 살펴보도록 하겠습니다.

### 소개

GPT-2 모델은 내용을 주장하고 만들어졌습니다.

- 여러 NLP 테스크에 대해서 각각의 데이터셋과 모델을 만들어 해결하는 것이 lack of generalization 이다.
- 각 테스크에 대해서 데이터셋을 만들지 않고 하나의 모델에서 해결할 수 있는 general system을 지향한다.
- 하나의 언어 모델이 zero-shot으로 down-stream 테스크를 모델 파라메터 수정없이 하도록 한다.

이렇게 이야기할 수 있는 것은 다음의 원문에 이렇게 나와있습니다.

```
    Since the supervised objective is the the same as the unsupervised objective but only evaluated on a subset 
    of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised 
    objective.

    번역: supervised objective 는 unsuperviesed objective와 동일하지만 순서의 하위 집합에서만 평가되기 때문에, 
         unsupervised objective의 전역 최소값은 supervised objective 의 전역 최소값이기도 하다.
```

말인 즉슨, "unsupervised objective를 가지고 모델학습을 하면 supervised objective에 대해서도 만족한다." 정도로 설명할 수 있을것 같습니다.

논문에서는 이를 가정하고 만든 GPT-2 모델을 통해 언어를 추론하고 NLP 테스크를 설명을 통해 수행할 수 있는 가능성을 확인하고자 했습니다.

### 접근

제안하는 GPT-2 모델에서는 여러가지 NLP 테스크가 가능해야 합니다. 그렇기 때문에 단순 텍스트 입력만 들어가는 것이 아니라 테스크에 대한 정보도 같이 들어갈 수 있어야 합니다. 만약에 모델에 입력한다면 (테스크 설명, 입력, 출력) 형태로 해서 (번역, 한국어 입력, 영어 출력) 또는 (QA, 컨텍스트, 질문, 답) 이렇게 작성을 할 것입니다. 논문의 예시로 보면 다음과 같이 볼 수 있겠습니다.

```
”I’m not the cleverest man in the world, but like they say in French: Je ne suis pas un imbecile [I’m not a fool].

In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate in the riding of Joliette, wrote in French: 
”Mentez mentez, il en restera toujours quelque chose,” which translates as, ”Lie lie and something will 
always remain.”

“I hate the word ‘perfume,”’ Burr says. ‘It’s somewhat better in French: ‘parfum.’

If listened carefully at 29:55, a conversation can be heard between two guys in French: 
“-Comment on fait pour aller de l’autre cote ́? -Quel autre cote ́?”, which means 
“- How do you get to the other side? - What side?”.

If this sounds like a bit of a stretch, consider this ques- tion in French: 
As-tu aller au cine ́ma?, or Did you go to the movies?, which literally translates as 
Have-you to go to movies/theater?

“Brevet Sans Garantie Du Gouvernement”, translated to English: “Patented without government warranty”.
```

예시는 WebText 학습 데이터 셋에서 영어에서 프랑스어로 변환되는 내용입니다. 모델에서 실험을 할 때는 위와 같이 앞에서 번역하고자 하는 문구를 입력하고 "wrote in French"나 "French:", "English:"와 같이 표기를 해서 변역된 문구가 생성되는 것을 기대할 것입니다.

이와 같은 방식은 McCann et al.(2018) 의 논문에서 MQAN에서 이미 다른 테스크에 대한 예시 포멧을 입력하여 NLU 테스트 수행하는 실험을 진행 했습니다.

### 데이터 

위에서 언급했다시피 자연어의 모든 테스크를 한 모델에서 할 수 있는 general system을 만들려면 아주 다양하고 많은 자연어 데이터를 수집 해야 합니다. 기존에 활용했던 위키피디아 데이터나 fiction books와 같은 소설 데이터를 많이 활용했는데 지금은 이보다 더 많고 다양한 자연어 리소스를 찾아야 합니다.

Common Crawl은 아마도 실험할 당시에 가장 접근하기 쉬운 다양하고 많은 자연어 데이터 중 하나인 듯 합니다. 하지만 이전에 다른 시험들에서 "whose content are mostly unintelligible"이라 불리는 데이터였는데 이러한 이슈를 GPT-2 실험 과정에서도 겪었다고 합니다. 결국 자체적을 WebText라는 데이터셋을 수집 및 구축했고 다음의 방식으로 데이터 크롤링을 진행했다고 합니다.

- 레딧에서 적어도 좋아요 3 개 정도 받은 보이는 링크들을 수집 (이 방식은 사용자들의 흥미나 지식적으로 도움이 되는 정도를 보는 휴리스틱한 지표 정도라 보면 된다고 합니다.)
- 다른 데이터에 비해 일반적이고 train, validation 시 데이터 overlapping으로 분석이 어려울 것을 고려하여 모든 위키피디아 문서 정보는 사용하지 않음

### 토크나이저

GPT-2에서는 Byte-level Byte Pair Encoding (BPE)를 활용합니다. 

### 모델

여기서도 이전 GPT 모델과 동일하게 Transformer 모델을 활용합니다. 그리고 다음의 변화를 약간 줍니다.

- Attention Layer에 대해서 LayerNorm을 Attention과 Feedforward의 출력부가 아닌 입력부에 적용합니다.
- 그리고 residual layer의 파라메터를 initialization 시에 1/root(N)으로 scale weight을 적용합니다. (여기서 N은 residual layer 수 입니다.)
- 추가로 context 길이를 1024로 batch size는 512를 활용합니다.

#### LayerNorm 위치는 왜?

이 부분에 대한 실험은 Xiong et al. "On Layer Normalization in the Transformer Architecture" 에서 진행 되었는데 LayerNorm을 Attention과 Feedforward 출력부에 두는 것보다 입력부에 두는 것이 학습 시 레이어별 Gradient의 정도가 상대적으로 고른 편이라 합니다. 

<img width="1023" alt="스크린샷 2021-05-30 오후 12 38 48" src="https://user-images.githubusercontent.com/16576921/120091235-3a996380-c144-11eb-8240-1c5879a0158c.png">


### 학습

실험을 위해 여기서는 GPT-2 모델을 위한 실험 모델 구성은 다음과 같이 구성합니다. 

<img width="469" alt="스크린샷 2021-05-30 오후 12 41 44" src="https://user-images.githubusercontent.com/16576921/120091256-69173e80-c144-11eb-8b21-ab32ba6c9835.png">

학습 시 큰 모델이기 때문에 learning rate 설정을 WebText 데이터의 5%로 미리 잡고 본 모델의 학습을 시작합니다. 논문에서는 여전히 모델이 underfit인 상황이고 학습을 하고 있다 합니다.

### Language Modeling

GPT-2를 학습하고 Byte-level BPE를 사용하였기 때문에 대부분의 토큰들 중 UNK 토큰이 발생할 가능성은 거의 없다 보면 됩니다. (실제로 40조 바이트 중 26번 정도 UNK가 발생했다 합니다.) 대부분의 단어가 생성이 가능하고 BPE를 통해 dotokenize (tokenize을 거꾸로 연산)도 가능합니다. WebText로 학습한 GPT-2모델은 detokenize 기준으로 2.5~5 사이의 PPL을 가졌다 합니다.

뿐만 아니라 WebText로 학습한 GPT-2 모델은 전반적인 도메인과 데이터셋에 잘 맞는 모델임을 다음 7개의 테스크에서 보입니다. 비록 One Billion Word Benchmark에서 저조한 성능을 보였지만 그 외의 태스크에서는 zero-shot learning이 어느정도 된다는 것을 보이고 있습니다.

<img width="1035" alt="스크린샷 2021-05-30 오후 12 42 40" src="https://user-images.githubusercontent.com/16576921/120091273-89df9400-c144-11eb-9036-6539aab83f5a.png">

### Children's Book Test

<img width="505" alt="스크린샷 2021-05-30 오후 12 43 36" src="https://user-images.githubusercontent.com/16576921/120091291-ad0a4380-c144-11eb-85d7-520d69a968f5.png">

텍스트 북에 빠진 개체명이나 명사에 대해서 모델이 예측하도록 하는 실험입니다. 예측 방식은 빠진 단어에 대해 가능한 10개의 선택에서 예측을 할 수 있도록 합니다. 이 데이터 중에 정글북과 같은 책은 제외가 되었는데 학습 데이터에도 같은 내용이 존재하여 실험에서 사용하지 않았습니다. 이 실험에서는 명사 예측에 대해서는 93.3% 그리고 일반 명사에 대해서는 89.1%의 Accuracy를 보였습니다.

### LAMBADA

간단하게 설명하면 긴 문장에 대해서 완벽하게 생성을 잘해내는지를 보는 실험입니다. 최소 50개의 단어 위치에서 마지막 단어를 예측하는 실험입니다. GPT-2 모델은 PPL 기존 SOTA 점수인 99.8에서 8.6으로 높은 성능을 보였고 accuracy는 19%에서 52.66%로 올라간것을 볼 수 있습니다. 만약에 마지막 단어를 강제하는 방식 (stop-word filter라 논문에서는 이야기 합니다)을 적용하면 63.24%까지 올라가는 것을 볼 수 있습니다.

### Winograd Schema Challenge

<img width="508" alt="스크린샷 2021-05-30 오후 12 44 54" src="https://user-images.githubusercontent.com/16576921/120091317-d925c480-c144-11eb-9de3-74dfc7cd8a4a.png">

튜링 테스트에서 많이 나오는 테스크입니다. 사람들이 생각하기에 일반적으로 생각하는 commonsense 문제를 잘 푸는지를 살펴보는 것이고 텍스트의 애매한 내용(ambiguities) 을 보고 예측하는 실험입니다. GPT-2는 partial score에서 70.7%의 accuracy를 보이며 기존의 SOTA를 넘어서는 것을 확인할 수 있습니다. 비록 적은 데이터셋으로 테스트를 하였다는 점은 고려를 할 필요는 있지만 어느정도 LM이 commonsense에 대한 문제를 잘 풀수 있는 것을 보였다는 점에서는 좋은 점 같습니다.

### Reading Comprehension

여러 기계 독해 테스크가 있는데 여기선 CoQA를 활용했습니다. 이 데이터는 기존에 많이 활용하는 SQuAD 데이터와 달리 긴 지식 컨텍스트가 주어지고 Question Answering을 대화 형태로 여러턴을 거처서 대화를 하여 정답을 맞추는 테스크이고 지식과 대화의 컨텍스트 두 가지를 고려하여 정답을 예측해야하는 실험입니다. 여기서 GPT-2는 Baseline과 비교했을 때 성능이 어떠한지를 보이고 있습니다. GPT-2 모델은 여기서 55 F1 score를 보입니다. 이 성능은 CoQA의 Baseline 모델의 성능과 비슷합니다. 학습을 하지 않고 zero shot으로 이러한 성능을 낼 수 있다는 것 자체로 의미가 있는 점수로 보입니다.

### Summarization

<img width="504" alt="스크린샷 2021-05-30 오후 12 46 04" src="https://user-images.githubusercontent.com/16576921/120091331-01adbe80-c145-11eb-9a99-a978cbc3f8fb.png">

GPT-2에서 생성 task를 하기 때문에 요약 테스크도 빠질 수 없는 것 같습니다. 여기서는 일반적으로 요약에서 많이 활용하는 CNN, Daily Mail dataset을 활용하여 요약문을 생성하도록 했습니다. GPT-2에서 zero-shot으로 생성하기 위해 TL;DR: 라는 annotation을 첨가해서 100개의 토큰을 생성했다 합니다. ROUGE를 가지고 메트릭으로 평가를 했으며 기사에서 random 3개의 문장을 뽑은 문장보다 훨씬 더 나은 요약문을 만들어 낼수 있는 것을 확인할 수 있었다 합니다.

### Translation

번역도 마찬가지로 실험으로 진행을 했습니다. WMT-14 데이터에서 English-French 그리고 그 역에 대해서도 진행을 했는데 각 성능은 BLEU score로 5, 11.5로 나왔습니다. Unsupervised로 학습한 MT 모델 점수 (33.5 BLEU)에 미치지는 못했지만 거의 영어 데이터로만 구성한 WebText에서 이러한 번역 결과가 나온점에 큰 의의를 두고 있습니다. 왜냐하면 프랑스어에 대한 정보가 WebText의 10MB 정도밖에 없었고 이것은 실제 French MT 말뭉치의 500분의 1정도라 합니다.

### Question Answering

<img width="1034" alt="스크린샷 2021-05-30 오후 12 47 20" src="https://user-images.githubusercontent.com/16576921/120091341-315cc680-c145-11eb-8521-11730cb95e0f.png">

기계 독해하고는 다르게 단문 단답을 하는 형태로 진행하는 실험입니다. 예를 들어 "조선시대에 한글을 창제한 임금은 누구지?"라 하면 GPT-2는 입력된 텍스트 질문 정보 안에서 "세종대왕"이라는 정답이 나와야 하는 구조 입니다. 여기서는 SQuAD Exact Match 기준 4.1%의 성능을 보였다 합니다. 작은 모델에서는 단 1%도 되지 않는 성능을 냈었는데 이의 5배의 성능을 낸 것으로도 유의미 하다 볼 수 있겠습니다.

### 생성일까요? 기억하는걸까요?

GPT-2 모델은 충분히 zero-shot learning으로 여러 태스크에서 괜찮은 성능을 보였습니다. 하지만 학습한 데이터 양이 방대하기 때문에 기존 태스크의 테스트와 overlap이 될 가능성의 여지도 역시 있을 수 있습니다. 만약에 overlap이 크다면 큰 모델은 결국 생성이 아닌 모델 파라메터에서 기억하는 내용을 그대로 출력하는 것으로 생각할 수 있습니다. 논문에서는 이러한 점에 대해서 하나의 컬럼으로 다루면서 GPT-2 모델이 단순히 데이터 학습으로 기억된 정보만을 가지고 답을 하는 것이 아닌 것을 보이려 하고 있습니다. 

이 논문에서는 overlap의 기준을 8-grams overlap이라는 기준으로 overlap이 된 정도를 보고 있습니다. 이를 Bloom filters라고 이야기 합니다. (이 기준은 overlap의 false positive rate 기준으로 보았고 8-gram의 upper boundary가 1e-8정도였다 합니다.)

이 Bloom filters 방식으로 자신들이 학습한 WebText는 overlap이 상대적으로 덜 되었다는 점을 보이려 합니다. 먼저 Common Text라 볼 수 있는 PTB, WikiText와 같은 데이터셋을 두고 각각의 학습 데이터와 테스트 데이터가 얼마나 overlap이 되는지를 보고 WebText의 학습데이터와 이들의 테스트 데이터와 얼마나 오버랩이 되는지 보입니다.

<img width="723" alt="스크린샷 2021-05-30 오후 12 48 04" src="https://user-images.githubusercontent.com/16576921/120091346-4a657780-c145-11eb-8546-0a48142c4c64.png">

오히려 Common Text에서 학습과 테스트 데이터의 overlap 비중이 컸고 WebText의 비중이 작은 것을 확인할 수 있었다 합니다. 더불어서 단순히 이렇게 자신들의 학습데이터와 테스트 데이터의 overlap 비중만 보이지 않고 실험에 적용한 테스크 간의 데이터 overlap의 정도를 파악하고 데이터를 제거해서 살펴보는 일종의 ablation test도 진행을 했습니다. 각 테스크 별로 일부 overlap이 되는 것을 확인할 수 있었고 이를 제외 했을 때 PPL이나 accuracy가 떨어지는 것을 확인했지만 그럼에도 적은 영향었다는 점에서 봤을 떄 overlap이 이슈가 아니고 모델이 생성함을 보이고 있다고 말하고 있습니다.

이렇게 논문에서는 데이터 overlap 이슈에 대해서 그렇게 큰 문제가 아니라는 점을 설명하면서 이 컬럼의 마지막에서는 여전히 GPT-2는 underfitting 상태이고 여전히 potential이라 언급하고 있습니다.

### 정리

논문에서는 GPT-2라는 모델을 소개했고 하나의 언어모델을 가지고 여러가지 테스크를 zero-shot으로 할 수 있는지를 실험하는 내용을 다루었습니다. 아직 이 논문에서는 여러 테스트에서 바로 활용할 수 있을만한 성능을 보이지는 못했지만 충분히 언어모델이 unsupervised learning을 통해 각 테스크에 대한 어느정도의 inference를 자체적으로 할 수 있다는 정도의 결과를 보인 셈인듯 합니다. 무엇보다도 여전히 모델 학습이 overfitting이 되지 않은 상황에서 실험을 했습니다. 그렇기 때문에 이 논문 저자들은 이후의 모델이 어떻게 나왔을지 기대를 했을 것이라 봤을 것이고 지금의 GPT-3 모델을 만들지 않았나 싶습니다.