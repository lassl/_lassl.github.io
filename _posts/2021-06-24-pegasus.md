---
layout: post
title: PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization
author: hyungjun.kim
categories: [paper]
tags: [Pegasus, Summarization]
---

# Introduction
Pretrain 모형이 발전하는 가운데, 요약(summarization)에 알맞은 pretrain 기법 중 하나인 Pegasus를 소개합니다. PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization 는 Abstractive Summarization 태스크를 위해 Extracted Gap-sentences 기법을 사용한 pre-training 모형입니다.

먼저 요약 태스크는 (1) extractive 방식과 (2) abstractive 방식으로 구분 됩니다. extractive 방식은 원문에서 주요한 문장들을 추출하는 방식이고, abstractive 방식은 원문을 추상적으로 파악해 원문에 없던 문장이라도 생성이 가능한 방식입니다.

위의 문단을 각각의 방식으로 요약해봅시다.

- Extractive Summarization: 먼저 요약 태스크는 (1) extractive 방식과 (2) abstractive 방식으로 구분 됩니다.
- Abstractive Summarization: 요약 테스크는 (1) 주요한 문장을 추출하는 extractive 방식과 (2) 추상적으로 요약하는 abstractive 방식이 있습니다.

이 논문의 주 된 Abstractive Summarization은 주요한 정보를 가지고 있으면서 언어적으로 유창해야합니다.




# Abstract
요약 태스크를 잘하기 위한 pre-training objective에 대한 고민을 하였고, Transformer-baed encoder-decoder를 self-supervised objectvie 방식으로 이용하였습니다. 12개의 요약 태스크를 평가해서 가장 좋은 성능을 보였고,
low-resource(약 1000개 문장)로 아주 빠르게 파인튜닝되어 SOTA의 성능을 보였다.

----

![img](https://user-images.githubusercontent.com/11376047/122936917-825e8400-d3ac-11eb-974a-acf5059c71e5.png)

위의 그림에서 볼 수 있듯이 Encoder에서는 일부 단어 (MASK2)로 MLM이 수행되고, 또한 중요한 문장 (MASK1)은 Encoder에서 마스크 된 후 Decoder에서 재성성 된다(Gap Sentences Generation). GSG는 전체 문서를 이해하고 요약처럼 생성하는 것에 도움이 된다. 즉, abstractive summarization objective에 적합한 방식이다.

# Related Work

- MASS: seq2seq 방식으로 enc의 일부 단어를 masking 후 decoder에서 재생성. 단일 문장에서 mask 부분은 랜덤하게 선택함.

![MASS](https://user-images.githubusercontent.com/11376047/122953875-ecc9f100-d3b9-11eb-9ad3-e9e5fd147c17.png)

- UniLM: 동시에 3가지 LM 태스크를 수행함. bidirectional (word-level mask, with next sentence prediction), unidirectional(left-to-right and right-to-left), and sequence-to-sequence (word-level mask) prediction.

![UniLM](https://user-images.githubusercontent.com/11376047/122954250-3dd9e500-d3ba-11eb-8e9a-1568b657889e.png)

- T5: 일반화된 text-to-text 프레임워크. 랜덤하게 다양한 비율로 텍스트 길이를 마스킹하여 pre-train.

![T5](https://1.bp.blogspot.com/-89OY3FjN0N0/XlQl4PEYGsI/AAAAAAAAFW4/knj8HFuo48cUFlwCHuU5feQ7yxfsewcAwCLcBGAsYHQ/s640/image2.png)

- BART: denoising autoencoder. [https://github.com/modulabs/beyondBERT/issues/6](https://github.com/modulabs/beyondBERT/issues/6) 참고

![BART](https://user-images.githubusercontent.com/17489107/86429243-edd9bc80-bd29-11ea-9080-2a275bc683f6.png)
![BART](https://user-images.githubusercontent.com/17489107/86429249-f6ca8e00-bd29-11ea-9889-f92a71fecc8d.png)

- PEGASUS: text spans에서 whole sentences로 재정의. 주요한 문장을 선택(extract)하여 마스킹하고 그 문장만 재생성함. NLU보다 요약에 조금 더 초점을 맞춤.


# Pre-training Objectives
단순히 문장을 copy하는 것을 넘어 주요한 문장들을 복원하게 합니다.

- Random: Uniformly m개의 문장 선택
- Lead: 처음부터 m개의 문장 선택
- Principal: 중요도에 따라 스코어가 높은 top-m 개의 문장 선택, $s_{i}= rouge(x_{i}, D \backslash \{x_{i}\}), ∀i$
    - selection
        - Ind(independently)
        - Seq(Sequentially)
    - ROUGE1-F1
        - Uniq: 반복되는 n-grams 중복 제거(Set 개념)
        - Orig: n-grams

![img](https://user-images.githubusercontent.com/11376047/123284049-e6b54b00-d546-11eb-9157-acc05709e8d5.png)

## References
- [https://github.com/microsoft/MASS](https://github.com/microsoft/MASS)
- [https://github.com/microsoft/unilm](https://github.com/microsoft/unilm)
- [https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
- [https://github.com/modulabs/beyondBERT/issues/6](https://github.com/modulabs/beyondBERT/issues/6)

# MLM
- BERT와 동일하게 15% 토큰 중 (1) 80% 마스킹, (2) 10% 무작위 토큰으로 교체 (3) 10% 그대로 사용. Decoder는 Encoder와 모든 파라미터 공유함.
실험 결과 MLM의 Marginal Effect는 없어서 최종적으로 $PEGASUS_{LARGE}$에서 MLM은 사용하지 않음.

# Pre-training Corpus
- C4: Common Crawl 350M Web-pages (750GB)
- HugeNews 1.5B articles (3.8TB)

# Downstream Tasks (TensorFlow Summarization Datasets)
- XSum
- CNN/DailyMail
- NEWSROOM
- Multi-News
- Gigaword
- arXiv, PubMed
- BIGPATENT
- wikiHow
- Reddit TIFU
- AESLC
- BillSum

# Experiments
![img](https://user-images.githubusercontent.com/11376047/123288558-a5bf3580-d54a-11eb-8750-0fbf8b152335.png)

## 6.1.2 Effect of Pre-training Objectives
![img](https://user-images.githubusercontent.com/11376047/123288943-f767c000-d54a-11eb-891c-beaf78d891a1.png)

- Lead: News에서는 효과 있으나, non-news에서는 효과가 좋지 못함
- Principal 방법들이 효과가 좋음(특히 Ind-Orig)
- GSR 비율을 높이면 context의 정보가 줄어들고, GSR 비율을 낮추면 너무 쉬워짐
- 학습 초기 (100k-200k)에는 MLM효과가 있었으나 학습이 진행되면서(500k) MLM gains가 사라짐

## 6.1.3. Effect of Vocabulary

![img](https://user-images.githubusercontent.com/11376047/123290295-1a46a400-d54c-11eb-977f-837c98bf8bef.png)

- Non-news Datasets에서 Unigram이 BPE에 비해 좋은 성능을 보임. Unigram 96k을 최종적으로 선택함.

## 6.2. Larger Model Results

- $PEGASUS_{BASE}$(223M) -> $PEGASUS_{LARGE}$(568M)
- GSG에서 20%는 마스킹 하지 않고 COPY되도록 그대로 사용
- GSG 비율을 45% 증가시킴

![img](https://user-images.githubusercontent.com/11376047/123295003-4bc16e80-d550-11eb-91d9-088f7af98cb1.png)

## 6.3. Zero and Low-Resource Summarization

- training set에서 처음 $10^{k}$만큼 데이터를 선택함.

![img](https://user-images.githubusercontent.com/11376047/123296753-e3738c80-d551-11eb-8cef-2f4785ca7580.png)

![img](https://user-images.githubusercontent.com/11376047/123297132-42390600-d552-11eb-92aa-41e875be3801.png)

## 6.5. Test-set Overlap with Pre-training Corpus

- XSum만 약 15% ~ 20% 겹침. 그러나, 중복 제거 후에도 ROUGE가 변하지는 않았음.

![img](https://user-images.githubusercontent.com/11376047/123298186-3568e200-d553-11eb-83bb-542f5f965d61.png)

## 6.6. Additional $PEGASUS_{LARGE}$ Improvements

- C4와 HugeNews 샘플 수에 따라 mixing
- 15% ~ 45% 사이에서 다이나믹 GSG
- 주요한 문장 추출 시 20% 노이징
- 500k steps -> 1.5M steps
- newline 추가

![img](https://user-images.githubusercontent.com/11376047/123299151-30586280-d554-11eb-9aa5-a631a61121ee.png)

