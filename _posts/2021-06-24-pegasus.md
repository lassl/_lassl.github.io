---
layout: post
title: PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization
author: hyungjun.kim
categories: [paper]
tags: [Pegasus, Summarization]
---

# Introduction
Pretrain 모형이 발전하는 가운데, 요약(summarization)에 알맞은 pretrain 기법 중 하나인 Pegasus를 소개합니다. PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization 는 Abstractive Summarization 태스크를 위해 Extracted Gap-sentences 기법을 사용한 pre-training 모형입니다.

먼저 요약 태스크는 (1) extractive 방식과 (2) abstractive 방식으로 구분 됩니다. extractive 방식은 원문에서 주요한 문장들을 추출하는 방식이고, abstractive 방식은 원문을 추상적으로 파악해 원문에 없던 문장이라도 생성이 가능한 방식입니다.

위의 문단을 각각의 방식으로 요약해봅시다.

- Extractive Summarization: 먼저 요약 태스크는 (1) extractive 방식과 (2) abstractive 방식으로 구분 됩니다.
- Abstractive Summarization: 요약 테스크는 (1) 주요한 문장을 추출하는 extractive 방식과 (2) 추상적으로 요약하는 abstractive 방식이 있습니다.

이 논문의 주 된 Abstractive Summarization은 주요한 정보를 가지고 있으면서 언어적으로 유창해야합니다.




# Abstract
요약 태스크를 잘하기 위한 pre-training objective에 대한 고민을 하였고, Transformer-baed encoder-decoder를 self-supervised objectvie 방식으로 이용하였습니다. 12개의 요약 태스크를 평가해서 가장 좋은 성능을 보였고,
low-resource(약 1000개 문장)로 아주 빠르게 파인튜닝되어 SOTA의 성능을 보였다.

----

![img](https://user-images.githubusercontent.com/11376047/122936917-825e8400-d3ac-11eb-974a-acf5059c71e5.png)

위의 그림에서 볼 수 있듯이 Encoder에서는 일부 단어 (MASK2)로 MLM이 수행되고, 또한 중요한 문장 (MASK1)은 Encoder에서 마스크 된 후 Decoder에서 재성성 된다(Gap Sentences Generation). GSG는 전체 문서를 이해하고 요약처럼 생성하는 것에 도움이 된다. 즉, abstractive summarization objective에 적합한 방식이다.

# Related Work

- MASS: seq2seq 방식으로 enc의 일부 단어를 masking 후 decoder에서 재생성. 단일 문장에서 mask 부분은 랜덤하게 선택함.

![MASS](https://user-images.githubusercontent.com/11376047/122953875-ecc9f100-d3b9-11eb-9ad3-e9e5fd147c17.png)

- UniLM: 동시에 3가지 LM 태스크를 수행함. bidirectional (word-level mask, with next sentence prediction), unidirectional(left-to-right and right-to-left), and sequence-to-sequence (word-level mask) prediction.

![UniLM](https://user-images.githubusercontent.com/11376047/122954250-3dd9e500-d3ba-11eb-8e9a-1568b657889e.png)

- T5: 일반화된 text-to-text 프레임워크. 랜덤하게 다양한 비율로 텍스트 길이를 마스킹하여 pre-train.

![T5](https://1.bp.blogspot.com/-89OY3FjN0N0/XlQl4PEYGsI/AAAAAAAAFW4/knj8HFuo48cUFlwCHuU5feQ7yxfsewcAwCLcBGAsYHQ/s640/image2.png)

- BART: denoising autoencoder. [https://github.com/modulabs/beyondBERT/issues/6](https://github.com/modulabs/beyondBERT/issues/6) 참고

![BART](https://user-images.githubusercontent.com/17489107/86429243-edd9bc80-bd29-11ea-9080-2a275bc683f6.png)
![BART](https://user-images.githubusercontent.com/17489107/86429249-f6ca8e00-bd29-11ea-9889-f92a71fecc8d.png)

- PEGASUS: text spans에서 whole sentences로 재정의. 주요한 문장을 선택(extract)하여 마스킹하고 그 문장만 재생성함. NLU보다 요약에 조금 더 초점을 맞춤.

## References
- [https://github.com/microsoft/MASS](https://github.com/microsoft/MASS)
- [https://github.com/microsoft/unilm](https://github.com/microsoft/unilm)
- [https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
- [https://github.com/modulabs/beyondBERT/issues/6](https://github.com/modulabs/beyondBERT/issues/6)