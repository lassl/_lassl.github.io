---
layout: post
title: Hydra
author: hyungjun.kim
categories: [implementation]
tags: [Hydra, Config, OmegaConf]
---

- 머신러닝 실험 등 복잡한 어플리케이션을 구성하기 위한 프레임워크
<br/>

---
## Introduction
하나의 완성된 모형을 만들기 위해서는 적어도 수십번의 실험을 하게 된다. 이 때 사용된 arguments와 결과값(예, metrics)의 쌍들(pairs)을 파일이나 엑셀로 관리 할 수도 있다. 그러나 arguments가 복잡할 수록 이를 구조화하거나 관리하기 힘들기 때문에 config 파일로 별도 관리하는 것이 좋다. 이 글에서는 Hydra에서 어떻게 config의 장점들을 강화하고, 반대로 단점들을 보완하고 있는지 살펴보자.

## Hydra의 Config 철학

#### 01. 변화에 민감할 것

config를 이용해 머신러닝 실험을 할 때 learning rate 등만 바꾸고 배치 사이즈 등 나머지는 동일할 경우 2개의 파일(configs)을 별도로 미리 가지고 있어야 할까? 만약 CLI(Command-Line Interface)에서 config에 원하는 값들만 덮어쓰기(override) 할 수 있으면 관리가 편할 것이다.

다음과 같은 config.yaml 이 있을 때

```yaml
app:
  task: classfication
  optimizer: adamw
```

아래와 같이 실행하면 override할 수 있다.

```sh
python my_app.py app.task=regression
```

다음은 config 출력 결과이다.

```yaml
app:
  task: regression
  optimizer: adamw
```

#### 02. Monolithic 구조

머신러닝 실험에서 `lightning-transformers` 예처럼 datasets, model, optimizer 등으로 구조화하여 실험을 할 수 있다. 또한 다양한 계층의 config를 구조화하여 통합 관리할 수 있다.

이를 위해 Hydra에서는 몇 가지 개념들을 도입하고 있다.
    - config group: 하위 그룹 config path
    - package: config node path

아래의 예시에서 config는 conf/config.yaml에 정의하고 있다.

conf/config.yaml
```yaml
defaults:
  - training: default
  - optimizer/adam_series: adamw
```

config group으로 training과 optimizer를 사용하여 계층을 지니고 있다. 기본값으로 training의 기본값과 optimizer는 adamW를 사용한다. optimizer는 adam_series를 폴더로 한 단 계 더 계층을 이루고 있다. 즉, config.yaml과 각각의 config yaml들을 nested 형태의 구조로 구성할 수 있다.

conf/training/default.yaml

```yaml
# @package training
lr: 1e-5
```

conf/optimizer/adam_series/adamw.yaml

```yaml
# @package optimizer
_target_: torch.optim.AdamW
lr: ${training.lr}
weight_decay: 0.001
```

또한, adamW에서는 training에 있는 lr을 package를 이용하여 변수로 값을 받을 수 있다. 그리고 _target_을 이용하여 AdamW 파이썬 객체를 해석 할 수 있다.

src/my_hydra.py
```python
@hydra.main(config_path='../conf/', config_name='config.yaml')
def my_app(cfg: DictConfig) -> None:
    print(OmegaConf.to_yaml(cfg))
    model = torch.nn.Sequential(
        torch.nn.Linear(3, 1),
        torch.nn.Flatten(0, 1)
    )
    opt = instantiate(cfg.optimizer, params=model.parameters())
```

> adamW를 pacakge를 사용하지 않고 optimzer로 표현하려면 _group_을 이용하여 표현 할 수 있다. `optimizer/adam_series: adamw` 대신 `optimizer@_group_: adam_series/adamw`가 사용되었다. 자세한 용례는 `https://github.com/facebookresearch/hydra/blob/master/website/versioned_docs/version-1.0/advanced/overriding_packages.md` 참조

```yaml
defaults:
  - /optimizer@_group_: adam_series/adamw
  - training: default
```

#### 03. Interpolation

일반적으로 python에서 yaml을 로드하면 파이썬 객체는 바로 평가할 수 없다. 물론 pickle 등으로 serialization 및 deserialization를 할 수 있지만 serialization 시의 파이썬 환경에 의존하는 등 불편한 점이 있다. Hydra에서는 OmegaConf를 통해 이를 해결하고 있다.

위의 02 예제 `conf/optimizer/adam_series/adamw.yaml`처럼 ${training.lr} 변수는 config를 통해 설정할 수 있다. 이는 평가 시 lazy evaluation을 하기 떄문에 가능하다.

----

위의 내용들을 `lightning-transformers`을 사례를 보면서 정리해보자.


```
conf/
┣ backbone/  # Configs defining the backbone of the model/pre-trained model if any
┣ dataset/ # Configs defining datasets
┣ optimizer/ # Configs for optimizers
┣ scheduler/ # Configs for schedulers
┣ task/ # Configs defining the task, and any task specific parameters
┣ tokenizer/ # Configs defining tokenizers, if any.
┣ trainer/ # Configs defining PyTorch Lightning Trainers, with different configurations
┣ training/ # Configs defining training specific parameters, such as batch size.
┗ config.yaml # The main entrypoint containing all our chosen config components
```

먼저 가장 기본이 되는 `config.yaml`을 정의한다. `lightning-transformers`에서는 `conf/config.yaml`에서 defaults로 task, optimizer, scheduler, training, trainer로 하위 계층 구조를 정의하고 있다.

```yaml
defaults: # loads default configs
  - task: default
  - optimizer: adamw
  - scheduler: linear_schedule_with_warmup
  - training: default
  - trainer: default

experiment_name: ${now:%Y-%m-%d}_${now:%H-%M-%S}
log: False
ignore_warnings: True # todo: check warnings before release
```

> 주) `now`는 hydra에서 미리 등록되어 사용할 수 있다. https://github.com/facebookresearch/hydra/blob/master/hydra/core/utils.py#L185

`conf/task/default.yaml`를 살펴보면, `package`와 `_group_` 개념이 사용되고 있다. dataset의 config group을 default로 설정하였기 때문에, `conf/dataset/default.yaml` 을 로드한다.

```yaml
# @package task
defaults:
  - /dataset@_group_: default

# By default we turn off recursive instantiation, allowing the user to instantiate themselves at the appropriate times.
_recursive_: false

_target_: lightning_transformers.core.model.TaskTransformer
optimizer: ${optimizer}
scheduler: ${scheduler}
```

> `_recursive_` 는 해당 파일에 종속된 다른 config들을 instantiate 할 것인지를 나타낸다.

본 블로그는 `hydra-core==1.1.0rc1`, `lightning-transformers==0.1` 버전을 기준으로 작성되었다.


## References
- https://hydra.cc/docs/intro/
- https://hydra.cc/docs/next/advanced/terminology/#config-group
- https://github.com/PyTorchLightning/lightning-transformers/blob/master/docs/source/structure/conf.rst
- https://medium.com/pytorch/hydra-a-fresh-look-at-configuration-for-machine-learning-projects-50583186b710
- https://github.com/facebookresearch/hydra/blob/master/website/versioned_docs/version-1.0/advanced/overriding_packages.md
- https://majianglin2003.medium.com/python-omegaconf-a33be1b748ab